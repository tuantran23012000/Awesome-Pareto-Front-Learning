# Awesome Pareto Front Learning
A collection of Awesome papers on Pareto Front Learning, Multiple Gradient Descent methods to solve Multi-Objective Optimization, Multi-Task Learning as Multi-Objective Optimization, and Optimization in Pareto Set.

## Paper
### Multiple Gradient Descent
 - Stochastic Multiple Target Sampling Gradient Descent [[NeurIPS 2022](https://arxiv.org/abs/2206.01934?fbclid=IwAR0DctSaeZhpvgJeYZO1RNCxCy4DR-PSB65qKOFklALv2rCyUw6W2sNAssw)] [[code](https://github.com/VietHoang1512/MT-SGD)]
 - Profiling Pareto Front With Multi-Objective Stein Variational Gradient Descent [[NeurIPS 2021 (Spotlight)](https://proceedings.neurips.cc/paper/2021/file/7bb16972da003e87724f048d76b7e0e1-Paper.pdf)] [[code](https://github.com/gnobitab/MultiObjectiveSampling)]
 - Multi-Task Learning with User Preferences Gradient Descent with Controlled Ascent in Pareto Optimization [[ICML 2020](http://proceedings.mlr.press/v119/mahapatra20a/mahapatra20a.pdf)] [[code](https://github.com/dbmptr/EPOSearch)]
 - Conflict-Averse Gradient Descent for Multi-task Learning [[NeurIPS 2021](https://arxiv.org/pdf/2110.14048.pdf)] [[code](https://github.com/Cranial-XIX/CAGrad.git)]
 - Gradient Surgery for Multi-Task Learning [[NeurIPS 2020](https://arxiv.org/pdf/2001.06782.pdf)] [[code](https://github.com/WeiChengTseng/Pytorch-PCGrad.git)]
 - Multi-Task Learning as Multi-Objective Optimization [[NIPS 2018](https://arxiv.org/pdf/1810.04650.pdf)] [[code](https://github.com/isl-org/MultiObjectiveOptimization)]
 - Towards Impartial Multi-task Learning [[ICLR 2021](https://openreview.net/pdf?id=IMPnRXEWpvr)] [[code](https://github.com/AvivNavon/nash-mtl.git)]
 <!-- - Fast Line Search for Multi-Task Learning [[arxiv 2021](https://arxiv.org/abs/2110.00874)] -->
 <!-- - A Hybrid 2-stage Neural Optimization for Pareto Front Extraction [[arxiv 2021](https://arxiv.org/abs/2101.11684)] [[code](https://openreview.net/attachment?id=UOj0MV__Cr&name=supplementary_material)] -->
 <!-- - Scalable Unidirectional Pareto Optimality for Multi-Task Learning with Constraints [[arxiv 2021](https://arxiv.org/abs/2110.15442)] -->

### Multi-Task Learning as Multi-Objective Optimization
 - Multi-Task Learning as Multi-Objective Optimization [[NIPS 2018](https://arxiv.org/pdf/1810.04650.pdf)] [[code](https://github.com/isl-org/MultiObjectiveOptimization)]
- Controllable Pareto Multi-Task Learning [[Arxiv 2021](https://arxiv.org/pdf/2010.06313.pdf)] [[code](https://openreview.net/attachment?id=5mhViEOQxaV&name=supplementary_material)]
 - Pareto Multi-Task Learning  [[NeurIPS 2019](https://proceedings.neurips.cc/paper/2019/file/685bfde03eb646c27ed565881917c71c-Paper.pdf)] [[code](https://github.com/Xi-L/ParetoMTL)]
 - Multi-Task Learning as a Bargaining Game  [[ICML 2022](https://arxiv.org/pdf/2202.01017.pdf)] [[code](https://github.com/AvivNavon/nash-mtl.git)]
 - A Multi-objective / Multi-task Learning Framework Induced by Pareto Stationarity [[ICML 2022](https://proceedings.mlr.press/v162/momma22a.html)]
 

### Pareto Set/Pareto Front Learning
 <!-- - Follow the bisector: a simple method for multi-objective optimization github [[arxiv 2020](https://arxiv.org/abs/2007.06937)] [[code](https://github.com/amkatrutsa/edm)] -->
 - Multi-Objective Learning to Predict Pareto Fronts Using Hypervolume Maximization [[Arxiv 2021](https://arxiv.org/pdf/2102.04523.pdf)] [[code](https://github.com/timodeist/multi_objective_learning)]
 - Self-Evolutionary Optimization for Pareto Front Learning [[Arxiv 2021](https://arxiv.org/pdf/2110.03461.pdf)]
 - Learning the Pareto Front with Hypernetworks [[ICLR 2021](https://arxiv.org/pdf/2010.04104.pdf)][[code](https://github.com/AvivNavon/pareto-hypernetworks.git)]
 - Improving Pareto Front Learning via Multi-Sample Hypernetworks [[AAAI 2023](https://arxiv.org/pdf/2212.01130.pdf)] [[code](https://github.com/longhoangphi225/MultiSample-Hypernetworks.git)]
 - Pareto Set Learning for Neural Multi-Objective Combinatorial Optimization [[ICLR 2022](https://arxiv.org/pdf/2203.15386.pdf)][[code](https://github.com/Xi-L/PMOCO.git)]
 - Pareto Set Learning for Expensive Multi-Objective Optimization [[NeurIPS 2022](https://arxiv.org/pdf/2203.15386.pdf)][[code](https://github.com/Xi-L/PSL-MOBO.git)]
 - Scalable Pareto Front Approximation for Deep Multi-Objective Learning [[ICDM 2021](https://arxiv.org/pdf/2103.13392.pdf)] [[code](https://github.com/ruchtem/cosmos)]
 - Effcient Continuous Pareto Exploration in Multi-Task Learning [[ICML 2020](http://proceedings.mlr.press/v119/ma20a/ma20a.pdf)] [[code](https://github.com/mit-gfx/ContinuousParetoMTL)]

### Optimization in Pareto Set
 - Pareto Navigation Gradient Descent: a First-Order Algorithm for Optimization in Pareto Set [[UAI 2022](https://arxiv.org/pdf/2110.08713)][[code](https://openreview.net/attachment?id=tiKNfYpH8le&name=supplementary_material)]
 - Pareto Efficient Fairness in Supervised Learning: From Extraction to Tracing [[Arxiv 2021](https://arxiv.org/pdf/2104.01634.pdf)]
<!-- ### Conference
 - Learning with Privileged Tasks [[ICCV 2021](https://openaccess.thecvf.com/content/ICCV2021/html/Song_Learning_With_Privileged_Tasks_ICCV_2021_paper.html)] 
 - A Multi-objective / Multi-task Learning Framework Induced by Pareto Stationarity [[ICML 2022](https://proceedings.mlr.press/v162/momma22a.html)] -->


